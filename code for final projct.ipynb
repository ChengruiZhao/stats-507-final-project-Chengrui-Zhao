{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b61ba18f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in d:\\anaconda3\\lib\\site-packages (3.1.0)\n",
      "Requirement already satisfied: transformers in d:\\anaconda3\\lib\\site-packages (4.47.0.dev0)\n",
      "Requirement already satisfied: accelerate in d:\\anaconda3\\lib\\site-packages (1.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in d:\\anaconda3\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in d:\\anaconda3\\lib\\site-packages (from datasets) (0.26.2)\n",
      "Requirement already satisfied: filelock in d:\\anaconda3\\lib\\site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in d:\\anaconda3\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\anaconda3\\lib\\site-packages (from datasets) (1.21.5)\n",
      "Requirement already satisfied: requests>=2.32.2 in d:\\anaconda3\\lib\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in d:\\anaconda3\\lib\\site-packages (from datasets) (18.0.0)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in d:\\anaconda3\\lib\\site-packages (from datasets) (4.66.6)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\anaconda3\\lib\\site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: fsspec[http]<=2024.9.0,>=2023.1.0 in d:\\anaconda3\\lib\\site-packages (from datasets) (2024.9.0)\n",
      "Requirement already satisfied: pandas in d:\\anaconda3\\lib\\site-packages (from datasets) (1.4.2)\n",
      "Requirement already satisfied: xxhash in d:\\anaconda3\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: packaging in d:\\anaconda3\\lib\\site-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: aiohttp in d:\\anaconda3\\lib\\site-packages (from datasets) (3.8.1)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in d:\\anaconda3\\lib\\site-packages (from transformers) (0.20.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\anaconda3\\lib\\site-packages (from transformers) (2022.3.15)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in d:\\anaconda3\\lib\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: torch>=1.10.0 in d:\\anaconda3\\lib\\site-packages (from accelerate) (2.5.1)\n",
      "Requirement already satisfied: psutil in d:\\anaconda3\\lib\\site-packages (from accelerate) (5.8.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in d:\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (4.0.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in d:\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in d:\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (2.0.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (21.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in d:\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (5.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in d:\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.6.3)\n",
      "Requirement already satisfied: typing-extensions>=3.6.5 in d:\\anaconda3\\lib\\site-packages (from async-timeout<5.0,>=4.0.0a3->aiohttp->datasets) (4.12.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in d:\\anaconda3\\lib\\site-packages (from packaging->datasets) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (3.3)\n",
      "Requirement already satisfied: sympy==1.13.1 in d:\\anaconda3\\lib\\site-packages (from torch>=1.10.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: jinja2 in d:\\anaconda3\\lib\\site-packages (from torch>=1.10.0->accelerate) (2.11.3)\n",
      "Requirement already satisfied: networkx in d:\\anaconda3\\lib\\site-packages (from torch>=1.10.0->accelerate) (2.7.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch>=1.10.0->accelerate) (1.2.1)\n",
      "Requirement already satisfied: colorama in d:\\anaconda3\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in d:\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.10.0->accelerate) (2.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in d:\\anaconda3\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\anaconda3\\lib\\site-packages (from pandas->datasets) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in d:\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: evaluate in d:\\anaconda3\\lib\\site-packages (0.4.3)\n",
      "Requirement already satisfied: xxhash in d:\\anaconda3\\lib\\site-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: packaging in d:\\anaconda3\\lib\\site-packages (from evaluate) (21.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in d:\\anaconda3\\lib\\site-packages (from evaluate) (0.26.2)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in d:\\anaconda3\\lib\\site-packages (from evaluate) (4.66.6)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in d:\\anaconda3\\lib\\site-packages (from evaluate) (2024.9.0)\n",
      "Requirement already satisfied: dill in d:\\anaconda3\\lib\\site-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: pandas in d:\\anaconda3\\lib\\site-packages (from evaluate) (1.4.2)\n",
      "Requirement already satisfied: datasets>=2.0.0 in d:\\anaconda3\\lib\\site-packages (from evaluate) (3.1.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in d:\\anaconda3\\lib\\site-packages (from evaluate) (2.32.3)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\anaconda3\\lib\\site-packages (from evaluate) (1.21.5)\n",
      "Requirement already satisfied: multiprocess in d:\\anaconda3\\lib\\site-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: filelock in d:\\anaconda3\\lib\\site-packages (from datasets>=2.0.0->evaluate) (3.6.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\anaconda3\\lib\\site-packages (from datasets>=2.0.0->evaluate) (6.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in d:\\anaconda3\\lib\\site-packages (from datasets>=2.0.0->evaluate) (18.0.0)\n",
      "Requirement already satisfied: aiohttp in d:\\anaconda3\\lib\\site-packages (from datasets>=2.0.0->evaluate) (3.8.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\anaconda3\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (5.1.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in d:\\anaconda3\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in d:\\anaconda3\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\anaconda3\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (21.4.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in d:\\anaconda3\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.6.3)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in d:\\anaconda3\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.0.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in d:\\anaconda3\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.5 in d:\\anaconda3\\lib\\site-packages (from async-timeout<5.0,>=4.0.0a3->aiohttp->datasets>=2.0.0->evaluate) (4.12.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in d:\\anaconda3\\lib\\site-packages (from packaging->evaluate) (3.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\anaconda3\\lib\\site-packages (from requests>=2.19.0->evaluate) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda3\\lib\\site-packages (from requests>=2.19.0->evaluate) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\anaconda3\\lib\\site-packages (from requests>=2.19.0->evaluate) (3.3)\n",
      "Requirement already satisfied: colorama in d:\\anaconda3\\lib\\site-packages (from tqdm>=4.62.1->evaluate) (0.4.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in d:\\anaconda3\\lib\\site-packages (from pandas->evaluate) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\anaconda3\\lib\\site-packages (from pandas->evaluate) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in d:\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install datasets transformers accelerate\n",
    "! pip install evaluate\n",
    "import evaluate\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c64b487",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c36d14855fd641f69d2b17cf058cb589",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "919e13df",
   "metadata": {},
   "outputs": [],
   "source": [
    "SUPERGLUE_TASKS = [\"boolq\",\"cb\",\"copa\",\"multirc\",\"record\",\"rte\",\"wic\",\"wsc.fixed\" ]\n",
    "task = \"boolq\"\n",
    "model_checkpoint = \"bert-base-uncased\"\n",
    "batch_size = 16\n",
    "import datasets\n",
    "actual_task = \"cb\" if task == \"copa\" else task\n",
    "dataset = datasets.load_dataset(\"super_glue\", actual_task, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c22806c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load('super_glue', actual_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca971681",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>passage</th>\n",
       "      <th>idx</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>are airsoft guns legal in new york state</td>\n",
       "      <td>Legal issues in airsoft -- During 1987 in New York City, more than 1400 toy imitation weapons involved in criminal acts were seized by New York City police; approximately 80 percent higher from the previous four years. On the basis of legislative intent dealing with the increasing volume of criminal acts in correlation with toy imitation weapons, New York City introduced new guidelines regulating the manufacture, importation, distribution, and sale of such imitation weapons. New York City requires that all realistic toy or imitation firearms be made of clear or brightly colored plastics. Furthermore, New York City makes possession of any pistol or rifle or similar instrument in which the propelling force is a spring or air unlawful without a license. See New York City Administrative Code § 10-131(b) and New York City Administrative Code § 10-131(g)(1)(a).</td>\n",
       "      <td>2014</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>do all business have to offer health insurance</td>\n",
       "      <td>Health insurance in the United States -- However, in a 2007 analysis, the Employee Benefit Research Institute concluded that the availability of employment-based health benefits for active workers in the US is stable. The ``take-up rate,'' or percentage of eligible workers participating in employer-sponsored plans, has fallen somewhat, but not sharply. EBRI interviewed employers for the study, and found that others might follow if a major employer discontinued health benefits. Effective by January 1, 2014, the Patient Protection and Affordable Care Act will impose a $2000 per employee tax penalty on employers with over 50 employees who do not offer health insurance to their full-time workers. (In 2008, over 95% of employers with at least 50 employees offered health insurance.) On the other hand, public policy changes could also result in a reduction in employer support for employment-based health benefits.</td>\n",
       "      <td>5929</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>is the united states located in the northern hemisphere</td>\n",
       "      <td>North America -- The term Northern America refers to the northern-most countries and territories of North America: the United States, Bermuda, St. Pierre and Miquelon, Canada and Greenland. Although the term does not refer to a unified region, Middle America--not to be confused with the Midwestern United States--groups the regions of Mexico, Central America, and the Caribbean.</td>\n",
       "      <td>5621</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>is a person's education level included in phi</td>\n",
       "      <td>Protected health information -- Protected health information (PHI) under the US law is any information about health status, provision of health care, or payment for health care that is created or collected by a Covered Entity (or a Business Associate of a Covered Entity), and can be linked to a specific individual. This is interpreted rather broadly and includes any part of a patient's medical record or payment history.</td>\n",
       "      <td>4658</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>are there such things as four leaf clovers</td>\n",
       "      <td>Four-leaf clover -- The four-leaf clover is a rare variation of the common three-leaf clover. According to traditional superstition, such clovers bring good luck, though it is not clear when or how that superstition got started. The earliest mention of ``Fower-leafed or purple grasse'' is from 1640 and simply says that it was kept in gardens because it was ``good for the purples in children or others''. A description from 1869 says that four-leaf clovers were ``gathered at night-time during the full moon by sorceresses, who mixed it with vervain and other ingredients, while young girls in search of a token of perfect happiness made quest of the plant by day''. The first reference to luck might be from an 11-year-old girl, who wrote in an 1877 letter to St. Nicholas Magazine, ``Did the fairies ever whisper in your ear, that a four-leaf clover brought good luck to the finder?''</td>\n",
       "      <td>8863</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>is calgary the largest city in north america</td>\n",
       "      <td>Calgary -- The city had a population of 1,239,220 in 2016, making it Alberta's largest city and Canada's third-largest municipality. Also in 2016, Calgary had a metropolitan population of 1,392,609, making it the fourth-largest census metropolitan area (CMA) in Canada.</td>\n",
       "      <td>6194</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>do iran and afghanistan speak the same language</td>\n",
       "      <td>Persian language -- Persian (/ˈpɜːrʒən, -ʃən/), also known by its endonym Farsi (فارسی fārsi (fɒːɾˈsiː) ( listen)), is one of the Western Iranian languages within the Indo-Iranian branch of the Indo-European language family. It is primarily spoken in Iran, Afghanistan (officially known as Dari since 1958), and Tajikistan (officially known as Tajiki since the Soviet era), and some other regions which historically were Persianate societies and considered part of Greater Iran. It is written in the Persian alphabet, a modified variant of the Arabic script, which itself evolved from the Aramaic alphabet.</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>is the statue of liberty located in the hudson river</td>\n",
       "      <td>Liberty Island -- Liberty Island is a federally owned island in Upper New York Bay in the United States, best known as the location of the Statue of Liberty. The island is an exclave of the New York City borough of Manhattan, surrounded by the waters of Jersey City, New Jersey. Long known as Bedloe's Island, it was renamed by an act of the United States Congress in 1956. In 1937, by Presidential Proclamation 2250 by President Franklin D. Roosevelt, it became part of the Statue of Liberty National Monument and in 1966, was listed on the National Register of Historic Places as part of Statue of Liberty National Monument, Ellis Island and Liberty Island.</td>\n",
       "      <td>6617</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>is bourbon street in the french quarter in new orleans</td>\n",
       "      <td>Bourbon Street -- Bourbon Street (French: Rue Bourbon, Spanish: Calle Bourbon) is a street in the heart of New Orleans' oldest neighborhood, the French Quarter, in New Orleans, Louisiana. It extends 13 blocks from Canal to Esplanade Avenue. Known for its bars and strip clubs, Bourbon Street's history provides a rich insight into New Orleans' past.</td>\n",
       "      <td>1377</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>can a baby survive outside the womb at 24 weeks</td>\n",
       "      <td>Fetal viability -- There is no sharp limit of development, gestational age, or weight at which a human fetus automatically becomes viable. According to studies between 2003 and 2005, 20 to 35 percent of babies born at 23 weeks of gestation survive, while 50 to 70 percent of babies born at 24 to 25 weeks, and more than 90 percent born at 26 to 27 weeks, survive. It is rare for a baby weighing less than 500 g (17.6 ounces) to survive. A baby's chances for survival increases 3-4% per day between 23 and 24 weeks of gestation and about 2-3% per day between 24 and 26 weeks of gestation. After 26 weeks the rate of survival increases at a much slower rate because survival is high already.</td>\n",
       "      <td>6834</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import datasets\n",
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def show_random_elements(dataset, num_examples=10):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "    \n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, datasets.ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "    display(HTML(df.to_html()))\n",
    "show_random_elements(dataset[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dbdbebd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.5625}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "fake_preds = np.random.randint(0, 2, size=(64,))\n",
    "fake_labels = np.random.randint(0, 2, size=(64,))\n",
    "metric.compute(predictions=fake_preds, references=fake_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c67e0327",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "    \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e11f7fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 7592, 1010, 2023, 2028, 6251, 999, 102, 1998, 2023, 6251, 3632, 2007, 2009, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"Hello, this one sentence!\", \"And this sentence goes with it.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf6dc083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1: do iran and afghanistan speak the same language\n",
      "Sentence 2: Persian language -- Persian (/ˈpɜːrʒən, -ʃən/), also known by its endonym Farsi (فارسی fārsi (fɒːɾˈsiː) ( listen)), is one of the Western Iranian languages within the Indo-Iranian branch of the Indo-European language family. It is primarily spoken in Iran, Afghanistan (officially known as Dari since 1958), and Tajikistan (officially known as Tajiki since the Soviet era), and some other regions which historically were Persianate societies and considered part of Greater Iran. It is written in the Persian alphabet, a modified variant of the Arabic script, which itself evolved from the Aramaic alphabet.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 2079, 4238, 1998, 7041, 3713, 1996, 2168, 2653, 102, 4723, 2653, 1011, 1011, 4723, 1006, 1013, 100, 1010, 1011, 1130, 29681, 2078, 1013, 1007, 1010, 2036, 2124, 2011, 2049, 2203, 16585, 2213, 2521, 5332, 1006, 1291, 25573, 17149, 29824, 24830, 2521, 5332, 1006, 1042, 29678, 23432, 29692, 29715, 5332, 23432, 1007, 1006, 4952, 1007, 1007, 1010, 2003, 2028, 1997, 1996, 2530, 7726, 4155, 2306, 1996, 11424, 1011, 7726, 3589, 1997, 1996, 11424, 1011, 2647, 2653, 2155, 1012, 2009, 2003, 3952, 5287, 1999, 4238, 1010, 7041, 1006, 3985, 2124, 2004, 18243, 2072, 2144, 3845, 1007, 1010, 1998, 23538, 1006, 3985, 2124, 2004, 11937, 4478, 3211, 2144, 1996, 3354, 3690, 1007, 1010, 1998, 2070, 2060, 4655, 2029, 7145, 2020, 4723, 3686, 8384, 1998, 2641, 2112, 1997, 3618, 4238, 1012, 2009, 2003, 2517, 1999, 1996, 4723, 12440, 1010, 1037, 6310, 8349, 1997, 1996, 5640, 5896, 1010, 2029, 2993, 7964, 2013, 1996, 28934, 12440, 1012, 102], [101, 2079, 2204, 3520, 8486, 5794, 4277, 4047, 2216, 2040, 2393, 2012, 2019, 4926, 102, 2204, 3520, 8486, 5794, 2375, 1011, 1011, 2204, 3520, 8486, 5794, 4277, 3749, 3423, 3860, 2000, 2111, 2040, 2507, 9608, 5375, 2000, 2216, 2040, 2024, 1010, 2030, 2040, 2027, 2903, 2000, 2022, 1010, 5229, 1010, 5665, 1010, 1999, 2566, 4014, 1010, 2030, 4728, 27523, 19498, 15198, 1012, 1996, 3860, 2003, 3832, 2000, 5547, 2011, 21515, 2545, 1005, 13431, 2000, 6509, 1010, 2005, 3571, 1997, 2108, 12923, 2030, 21651, 2005, 4895, 18447, 4765, 19301, 4544, 2030, 3308, 3993, 2331, 1012, 2019, 2742, 1997, 2107, 1037, 2375, 1999, 2691, 1011, 2375, 2752, 1997, 2710, 1024, 1037, 2204, 3520, 8486, 5794, 8998, 2003, 1037, 3423, 6958, 2008, 16263, 1037, 5343, 2099, 2040, 2038, 17912, 3271, 1037, 6778, 1999, 12893, 2013, 2108, 5147, 12923, 2005, 3308, 3527, 2075, 1012, 2049, 3800, 2003, 2000, 2562, 2111, 2013, 2108, 11542, 2000, 2393, 1037, 7985, 1999, 2342, 2005, 3571, 1997, 3423, 16360, 2121, 7874, 27466, 2323, 2027, 2191, 2070, 6707, 1999, 3949, 1012, 2011, 5688, 1010, 1037, 4611, 2000, 5343, 2375, 5942, 2111, 2000, 3749, 5375, 1998, 4324, 2216, 2040, 8246, 2000, 2079, 2061, 20090, 1012, 102], [101, 2003, 3645, 3185, 9338, 2112, 1997, 3645, 6827, 2015, 102, 3645, 3185, 9338, 1011, 1011, 3645, 3185, 9338, 1006, 3839, 2124, 2004, 3645, 2444, 3185, 9338, 1999, 3645, 1021, 1007, 2003, 1037, 8944, 2678, 9260, 4007, 2011, 7513, 1012, 2009, 2003, 1037, 2112, 1997, 3645, 6827, 2015, 4007, 7621, 1998, 4107, 1996, 3754, 2000, 3443, 1998, 10086, 6876, 2004, 2092, 2004, 2000, 10172, 2068, 2006, 2028, 23663, 1010, 9130, 1010, 6819, 26247, 1010, 7858, 1010, 1998, 17312, 2099, 1012, 102], [101, 2003, 9530, 25969, 3258, 5649, 5699, 1996, 2168, 2004, 9898, 2098, 5699, 102, 9898, 2098, 5699, 1011, 1011, 9898, 2098, 5699, 1010, 2036, 2170, 9530, 25969, 3258, 2545, 1005, 5699, 1010, 24582, 2075, 5699, 1010, 1998, 24582, 2075, 9850, 1010, 2003, 1037, 22126, 2598, 5699, 2550, 2011, 22491, 12604, 8898, 5699, 2046, 1037, 9898, 2098, 2110, 1012, 2009, 2788, 3397, 1037, 2235, 3815, 1997, 3424, 1011, 6187, 6834, 4005, 2000, 4652, 18856, 24237, 2075, 1998, 5335, 4834, 1012, 2348, 2087, 2411, 2550, 1999, 1037, 4713, 1010, 9898, 2098, 5699, 2064, 2036, 2022, 2081, 2011, 6364, 6623, 12604, 8898, 5699, 1999, 1037, 4157, 23088, 2121, 1010, 2030, 2011, 14527, 2009, 2011, 2192, 1999, 1037, 14335, 1998, 20739, 2571, 1012, 102], [101, 2003, 6422, 23074, 3784, 1996, 2168, 2004, 3712, 20026, 102, 1996, 6422, 23074, 3784, 1011, 1011, 2004, 2007, 2060, 2399, 1999, 1996, 6422, 23074, 2186, 1010, 1996, 2208, 2003, 2275, 2006, 1996, 9983, 1997, 17214, 7373, 2140, 1012, 1996, 2824, 1997, 1996, 2208, 5258, 1037, 10144, 2077, 2216, 1997, 1996, 6422, 23074, 1058, 1024, 3712, 20026, 1998, 2105, 5385, 2086, 2077, 1996, 6422, 23074, 3523, 1024, 19084, 22254, 1998, 1996, 6422, 23074, 4921, 1024, 24034, 1012, 2009, 2038, 1037, 13644, 2714, 3252, 2000, 3712, 20026, 1010, 2007, 2048, 3584, 9755, 27673, 2012, 1996, 2168, 2051, 1010, 2028, 2007, 1996, 6580, 1997, 1996, 2088, 1999, 1996, 5703, 1010, 1998, 2028, 2073, 1996, 3396, 2003, 4259, 2373, 2006, 17214, 7373, 2140, 1012, 1999, 1996, 6422, 23074, 3784, 1010, 1996, 2034, 5998, 2003, 2114, 1996, 4830, 2098, 7277, 3159, 9587, 17802, 28352, 1010, 2040, 2003, 7161, 2000, 11463, 2094, 1996, 4946, 1997, 14163, 4859, 2271, 2007, 2010, 8391, 1997, 3147, 8167, 25127, 1010, 1998, 1996, 2117, 2003, 2000, 5425, 1996, 10030, 4461, 6106, 1010, 7259, 2011, 2093, 21277, 1997, 1996, 9801, 3837, 1012, 1996, 2447, 2839, 2038, 2042, 20268, 2000, 9587, 17802, 28352, 1010, 1998, 9587, 17802, 28352, 2038, 7376, 2037, 3969, 1010, 1996, 7233, 1997, 2029, 2003, 1996, 3078, 2208, 7863, 1012, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_to_keys = {\n",
    "    \"boolq\": (\"question\", \"passage\"),\n",
    "    \"cb\": (\"premise\", \"hypothesis\"),\n",
    "    \"copa\": (\"premise\", \"choice1\"), \n",
    "    \"multirc\": (\"question\", \"paragraph\"),\n",
    "    \"record\": (\"passage\", \"query\"),\n",
    "    \"rte\": (\"sentence1\", \"sentence2\"),\n",
    "    \"wic\": (\"sentence1\", \"sentence2\"),  \n",
    "    \"wsc.fixed\": (\"text\", None)\n",
    "}\n",
    "sentence1_key, sentence2_key = task_to_keys[task]\n",
    "if sentence2_key is None:\n",
    "    print(f\"Sentence: {dataset['train'][0][sentence1_key]}\")\n",
    "else:\n",
    "    print(f\"Sentence 1: {dataset['train'][0][sentence1_key]}\")\n",
    "    print(f\"Sentence 2: {dataset['train'][0][sentence2_key]}\")\n",
    "def preprocess_function(examples):\n",
    "    if sentence2_key is None:\n",
    "        return tokenizer(examples[sentence1_key], truncation=True)\n",
    "    return tokenizer(examples[sentence1_key], examples[sentence2_key], truncation=True)\n",
    "preprocess_function(dataset['train'][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe74e397",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_dataset = dataset.map(preprocess_function, batched=True)\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "num_labels = 3 if task.startswith(\"mnli\") else 1 if task==\"stsb\" else 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f79ab27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ae6441a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\lib\\site-packages\\transformers\\training_args.py:1570: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_20144\\2915367881.py:38: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "metric_name = \"f1\" if task == \"multirc\" else \"f1\" if task == \"record\" else \"accuracy\"\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "\n",
    "args = TrainingArguments(\n",
    "    f\"{model_name}-finetuned-{task}\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=metric_name,\n",
    "    push_to_hub=True,\n",
    ")\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    if task in [\"cb\", \"rte\", \"boolq\", \"wic\", \"wsc.fixed\"]:\n",
    "        predictions = np.argmax(predictions, axis=1)\n",
    "    elif task == \"stsb\":\n",
    "        predictions = predictions[:, 0] \n",
    "    elif task in [\"multirc\", \"record\"]:\n",
    "        predictions = np.argmax(predictions, axis=-1) \n",
    "    elif task == \"copa\":\n",
    "        predictions = np.argmax(predictions, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "validation_key = {\n",
    "    \"boolq\": \"validation\",\n",
    "    \"cb\": \"validation\",\n",
    "    \"copa\": \"validation\",\n",
    "    \"multirc\": \"validation\",\n",
    "    \"record\": \"validation\",\n",
    "    \"rte\": \"validation\",\n",
    "    \"wic\": \"validation\",\n",
    "    \"wsc.fixed\": \"validation\",\n",
    "}.get(task, \"validation\")\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=encoded_dataset[\"train\"],\n",
    "    eval_dataset=encoded_dataset[validation_key],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f2f78ed7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2950' max='2950' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2950/2950 13:13:43, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.644500</td>\n",
       "      <td>0.588926</td>\n",
       "      <td>0.697248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.513900</td>\n",
       "      <td>0.568860</td>\n",
       "      <td>0.718349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.369500</td>\n",
       "      <td>0.704560</td>\n",
       "      <td>0.715902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.263200</td>\n",
       "      <td>0.955945</td>\n",
       "      <td>0.726300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.183700</td>\n",
       "      <td>1.065963</td>\n",
       "      <td>0.724771</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2950, training_loss=0.3567445451122219, metrics={'train_runtime': 47638.6766, 'train_samples_per_second': 0.989, 'train_steps_per_second': 0.062, 'total_flos': 7023126275745060.0, 'train_loss': 0.3567445451122219, 'epoch': 5.0})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0e8f6d3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='205' max='205' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [205/205 15:16]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.9559453725814819,\n",
       " 'eval_accuracy': 0.7262996941896025,\n",
       " 'eval_runtime': 918.5024,\n",
       " 'eval_samples_per_second': 3.56,\n",
       " 'eval_steps_per_second': 0.223,\n",
       " 'epoch': 5.0}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e6681bb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77c376e611864ca5b11a16439f380be4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "events.out.tfevents.1732287648.DESKTOP-VEQ2CRL:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/crzhao/bert-base-uncased-finetuned-boolq/commit/4f190cf4d0085e0a8fa33a790d4f7c39be4708fd', commit_message='End of training', commit_description='', oid='4f190cf4d0085e0a8fa33a790d4f7c39be4708fd', pr_url=None, repo_url=RepoUrl('https://huggingface.co/crzhao/bert-base-uncased-finetuned-boolq', endpoint='https://huggingface.co', repo_type='model', repo_id='crzhao/bert-base-uncased-finetuned-boolq'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad910ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install datasets transformers accelerate\n",
    "! pip install evaluate\n",
    "import evaluate\n",
    "import transformers\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()\n",
    "GLUE_TASKS = [\"cola\", \"mnli\", \"mnli-mm\", \"mrpc\", \"qnli\", \"qqp\", \"rte\", \"sst2\", \"stsb\", \"wnli\"]\n",
    "task = \"cola\"\n",
    "model_checkpoint = \"distilbert-base-uncased\"\n",
    "batch_size = 16\n",
    "import datasets\n",
    "actual_task = \"mnli\" if task == \"mnli-mm\" else task\n",
    "dataset = datasets.load_dataset(\"glue\", actual_task)\n",
    "metric = evaluate.load('glue', actual_task)\n",
    "import datasets\n",
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def show_random_elements(dataset, num_examples=10):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "    \n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, datasets.ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "    display(HTML(df.to_html()))\n",
    "show_random_elements(dataset[\"train\"])\n",
    "import numpy as np\n",
    "\n",
    "fake_preds = np.random.randint(0, 2, size=(64,))\n",
    "fake_labels = np.random.randint(0, 2, size=(64,))\n",
    "metric.compute(predictions=fake_preds, references=fake_labels)\n",
    "from transformers import AutoTokenizer\n",
    "    \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)\n",
    "tokenizer(\"Hello, this one sentence!\", \"And this sentence goes with it.\")\n",
    "task_to_keys = {\n",
    "    \"cola\": (\"sentence\", None),\n",
    "    \"mnli\": (\"premise\", \"hypothesis\"),\n",
    "    \"mnli-mm\": (\"premise\", \"hypothesis\"),\n",
    "    \"mrpc\": (\"sentence1\", \"sentence2\"),\n",
    "    \"qnli\": (\"question\", \"sentence\"),\n",
    "    \"qqp\": (\"question1\", \"question2\"),\n",
    "    \"rte\": (\"sentence1\", \"sentence2\"),\n",
    "    \"sst2\": (\"sentence\", None),\n",
    "    \"stsb\": (\"sentence1\", \"sentence2\"),\n",
    "    \"wnli\": (\"sentence1\", \"sentence2\"),\n",
    "}\n",
    "sentence1_key, sentence2_key = task_to_keys[task]\n",
    "if sentence2_key is None:\n",
    "    print(f\"Sentence: {dataset['train'][0][sentence1_key]}\")\n",
    "else:\n",
    "    print(f\"Sentence 1: {dataset['train'][0][sentence1_key]}\")\n",
    "    print(f\"Sentence 2: {dataset['train'][0][sentence2_key]}\")\n",
    "def preprocess_function(examples):\n",
    "    if sentence2_key is None:\n",
    "        return tokenizer(examples[sentence1_key], truncation=True)\n",
    "    return tokenizer(examples[sentence1_key], examples[sentence2_key], truncation=True)\n",
    "preprocess_function(dataset['train'][:5])\n",
    "encoded_dataset = dataset.map(preprocess_function, batched=True)\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "num_labels = 3 if task.startswith(\"mnli\") else 1 if task==\"stsb\" else 2\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels)\n",
    "metric_name = \"pearson\" if task == \"stsb\" else \"matthews_correlation\" if task == \"cola\" else \"accuracy\"\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "\n",
    "args = TrainingArguments(\n",
    "    f\"{model_name}-finetuned-{task}\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=metric_name,\n",
    "    push_to_hub=True,\n",
    ")\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    if task != \"stsb\":\n",
    "        predictions = np.argmax(predictions, axis=1)\n",
    "    else:\n",
    "        predictions = predictions[:, 0]\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "validation_key = \"validation_mismatched\" if task == \"mnli-mm\" else \"validation_matched\" if task == \"mnli\" else \"validation\"\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=encoded_dataset[\"train\"],\n",
    "    eval_dataset=encoded_dataset[validation_key],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "trainer.train()\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2e5553",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
